{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "import nodepy.linear_multistep_method as lm\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 2-D Harmonic Oscillator\n",
    "\n",
    "As a benchmark problem, the 2-D Cubic Harmonic Oscillator:\n",
    "\n",
    "$$ \\begin{array}\n",
    "\\dot{x_1} & = & -0.1 x_1^3 + 2.0 x_2^3 \\\\\n",
    "\\dot{x_2} & = & -2.0 x_1^3 - 0.1 x_2^3 \\\\\n",
    "\\end{array} $$\n",
    "\n",
    "with the initial conditions $x_1(0) = 2, x_2(0) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_cubic(x,t):\n",
    "    \"\"\"\n",
    "    Return the derivatives (RHS of the ODE)\n",
    "    This is a linear system with the form f = A x\n",
    "    Args:\n",
    "    x -- a 2 x 1 vector of measurements\n",
    "    \"\"\"\n",
    "    A = np.array([[-0.1, 2], [-2,-0.1]]) # 2 x 2\n",
    "\n",
    "    return np.ravel(np.matmul(A,x.reshape(-1, 1)**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create the training dataset by integrating the above system of equations to obtain the measurements. We also introduce a Gaussian noise to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time points\n",
    "time_points = np.arange(0,25,0.01)\n",
    "\n",
    "# specify initial conditions\n",
    "x0 = np.array([2,0])\n",
    "\n",
    "simulated_x = odeint(f, x0, time_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data\n",
    "\n",
    "noise = 0.00 #strength of the noise\n",
    "\n",
    "skip = 1\n",
    "dt = time_points[skip] - time_points[0]\n",
    "X_train = simulated_x[0::skip,:]\n",
    "X_train = X_train + noise * X_train.std(0) * np.random.randn(X_train.shape[0], X_train.shape[1])\n",
    "\n",
    "X_train = np.reshape(X_train, (1,X_train.shape[0],X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 2-D Yeast Glycolytic Oscillator\n",
    "\n",
    "The system of equations for this system is described in the notebook `Analysis - Data Exploration and Visualization`. Here we simulate the dataset and transform it into tensor for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bier(x,t, params=None):\n",
    "    \"\"\"\n",
    "    2-D Yeast Glycolytic oscillator model\n",
    "    \n",
    "    Args:\n",
    "        x -- a 2 x 1 vector of measurements\n",
    "        t -- time, ignored\n",
    "        \n",
    "    Return:\n",
    "        A numpy array containing the derivatives\n",
    "    \"\"\"\n",
    "    if params == None:\n",
    "        # default parameter values\n",
    "        Vin = 0.36\n",
    "        k1 = 0.02\n",
    "        kp = 6\n",
    "        km = 12\n",
    "    else:\n",
    "        Vin = params['Vin']\n",
    "        k1 = params['k1']\n",
    "        kp = params['kp']\n",
    "        km = params['km']\n",
    "    \n",
    "    r1 = 2 * k1 * x[0] * x[1] - kp * x[0]/(x[0] + km) # ATP\n",
    "    r2 = Vin - k1 * x[0] * x[1] #G\n",
    "    \n",
    "    return np.ravel(np.array([r1, r2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the settings for the bier data\n",
    "\n",
    "t0, T, h = 0, 500, 0.2 #seconds\n",
    "x0 = np.array([4, 3]) #initial conditions: ATP = 4, G = 3 -- default Bier model\n",
    "params = {'Vin': 0.36, 'k1': 0.02, 'kp':4, 'km':15} # damped oscillation\n",
    "f_bier = lambda x, t: bier(x, t, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(array):\n",
    "    \"\"\"\n",
    "    Create tensor array for training\n",
    "    \"\"\"\n",
    "    \n",
    "    training_data = np.reshape(array, (1,array.shape[0], array.shape[1]))\n",
    "    \n",
    "    return tf.convert_to_tensor(training_data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate bier data by solving the ODE\n",
    "time_points = np.arange(t0, T, h)\n",
    "bier_data = odeint(f_bier, x0, time_points)\n",
    "bier_data = create_training_data(bier_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Multistep Neural Network\n",
    "\n",
    "Define the Multistep Neural Network in TensorFlow and train it on the 2-D Bier data. We investigate the performance of the Multistep Neural Network for different number of steps and LMM schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "class lmmNet:\n",
    "    \"\"\"\n",
    "    Implementation of the LMMNet\n",
    "    version 1.2\n",
    "    Fixes/updates:\n",
    "        * number of hidden layer units is no longer hardcoded\n",
    "        * fixed bug for wrong indexing of the coefficients in computing linear diff operator\n",
    "        * loss printed every 100 epochs\n",
    "        * optimizer now declared in constructor\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h, X, M, scheme, hidden_units):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        h -- step size\n",
    "        X -- data array with shape S x N x D \n",
    "        M -- number of LMM steps\n",
    "        scheme -- the LMM scheme (either AB, AM, or BDF)\n",
    "        hidden_units -- number of units for the hidden layer\n",
    "        \n",
    "        \"\"\"\n",
    "        self.h = h\n",
    "        self.X = X\n",
    "        self.M = M # number of time steps\n",
    "        \n",
    "        # get the number of trajectories, discrete time instances, and number of feature dimensions\n",
    "        self.S = X.shape[0]\n",
    "        self.N = X.shape[1]\n",
    "        self.D = X.shape[2]\n",
    "        \n",
    "        # load LMM coefficients from NodePy\n",
    "        # https://nodepy.readthedocs.io/en/latest/\n",
    "        if scheme == 'AB':\n",
    "            coefs = lm.Adams_Bashforth(M)\n",
    "        elif scheme == 'AM':\n",
    "            coefs = lm.Adams_Moulton(M)\n",
    "        elif scheme == 'BDF':\n",
    "            coefs = lm.backward_difference_formula(M)\n",
    "        else:\n",
    "            raise Exception('Please choose a valid LMM scheme')\n",
    "        \n",
    "        self.alpha = np.float32(-coefs.alpha[::-1])\n",
    "        self.beta = np.float32(coefs.beta[::-1])\n",
    "        \n",
    "        class DenseModel(Model):\n",
    "            \"\"\"\n",
    "            A simple feed-forward network with 1 hidden layer\n",
    "            \n",
    "            Arch:\n",
    "            * 256 hidden units\n",
    "            * input units and output units correspond to the dimensionality\n",
    "            \"\"\"\n",
    "            def __init__(self, D):\n",
    "                super(DenseModel, self).__init__()\n",
    "                self.D = D\n",
    "\n",
    "                self.d1 = tf.keras.layers.Dense(units=hidden_units, activation=tf.nn.tanh, input_shape=(self.D,))\n",
    "                self.d2 = tf.keras.layers.Dense(units=self.D, activation=None)\n",
    "\n",
    "            def call(self, X1):\n",
    "                A = self.d1(X1)\n",
    "                A = self.d2(A)\n",
    "                return A\n",
    "        \n",
    "        self.nn = DenseModel(self.D)\n",
    "                \n",
    "        self.opt = tf.keras.optimizers.Adam()\n",
    "        \n",
    "    def get_F(self, X):\n",
    "        \"\"\"\n",
    "        Output of the NN/ML model.\n",
    "        \n",
    "        Args:\n",
    "        - X: the data matrix with shape S x (N-M) x D\n",
    "        \n",
    "        Output:\n",
    "        - F: the output dynamics with shape S x (N-M) x D\n",
    "        \"\"\"\n",
    "\n",
    "        assert X.shape == (self.S, self.N - self.M, self.D)\n",
    "        \n",
    "        X1 = tf.reshape(X, [-1, self.D])\n",
    "        F1 = self.nn(X1)\n",
    "        \n",
    "        return tf.reshape(F1, [self.S, -1, self.D])\n",
    "    \n",
    "    def get_Y(self, X):\n",
    "        \"\"\"\n",
    "        The linear difference (residual) operator.\n",
    "        \n",
    "        Args:\n",
    "        - X: the data matrix with shape S x N x D\n",
    "        \"\"\"\n",
    "        \n",
    "        M = self.M\n",
    "        \n",
    "        # compute the difference operator\n",
    "        # broadcasting from M to N to get an array for all n\n",
    "        # Y has shape S x (N - M) x D\n",
    "        Y = self.alpha[0] * X[:, M: ,:] + self.h * self.beta[0] * self.get_F(X[:, M:, :]) # for m = 0\n",
    "        \n",
    "        # sum over m from m = 1\n",
    "        for m in range(1, M+1):\n",
    "            Y += self.alpha[m] * X[:, M-m:-m, :] + self.h * self.beta[m] * self.get_F(X[:, M-m:-m, :])\n",
    "        \n",
    "        return self.D * tf.reduce_mean(tf.square(Y))\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        \"\"\"\n",
    "        Fit the model PyTorch-style\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.loss = self.get_Y(self.X)\n",
    "            grads = tape.gradient(self.loss, self.nn.trainable_variables)\n",
    "            self.opt.apply_gradients(zip(grads, self.nn.trainable_variables))\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                elapsed_time = timeit.default_timer() - start_time\n",
    "                #print('Epoch: %d, Time: %.2f, Loss: %.4e' %(epoch, elapsed_time, self.loss))\n",
    "                #tf.print(self.loss)\n",
    "\n",
    "        \n",
    "    def predict(self, X_reshaped):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - X_reshaped with shape S(N-M+1) x D\n",
    "        \"\"\"\n",
    "        return self.nn(X_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_f(x, t, model):\n",
    "    \"\"\"\n",
    "    Define the derivatives (RHS of the ODE) learned by ML\n",
    "    I think this is the best implementation (more robust than flatten())\n",
    "    \"\"\"\n",
    "    return np.ravel(model.predict(x.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MultistepNet for M =  1 on family =  AM\n",
      "WARNING:tensorflow:Layer dense_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  2 on family =  AM\n",
      "WARNING:tensorflow:Layer dense_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  3 on family =  AM\n",
      "WARNING:tensorflow:Layer dense_model_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  4 on family =  AM\n",
      "WARNING:tensorflow:Layer dense_model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  5 on family =  AM\n",
      "WARNING:tensorflow:Layer dense_model_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  1 on family =  AB\n",
      "WARNING:tensorflow:Layer dense_model_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  2 on family =  AB\n",
      "WARNING:tensorflow:Layer dense_model_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  3 on family =  AB\n",
      "WARNING:tensorflow:Layer dense_model_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  4 on family =  AB\n",
      "WARNING:tensorflow:Layer dense_model_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training MultistepNet for M =  5 on family =  AB\n"
     ]
    }
   ],
   "source": [
    "# train LMM with different schemes and number of steps\n",
    "M_list = [1, 2, 3, 4, 5]\n",
    "scheme_list = ['AM', 'AB', 'BDF']\n",
    "\n",
    "# for storing results\n",
    "result_dict = {}\n",
    "result_dict['data'] = bier_data\n",
    "result_dict['t'] = time_points\n",
    "\n",
    "for scheme in scheme_list:\n",
    "    for M in M_list:\n",
    "        print('Training MultistepNet for M = ', M, 'on family = ', scheme)\n",
    "        model = lmmNet(h, bier_data, M, scheme, hidden_units=256)\n",
    "        model.train(10000)\n",
    "        \n",
    "        pred = odeint(ml_f, x0, time_points, args=(model,))\n",
    "        \n",
    "        # store the result\n",
    "        result_dict[scheme + str(M) + 'pred'] = pred\n",
    "        result_dict[scheme + str(M) + 'f'] = [ml_f(x, None, model) for x in np.squeeze(bier_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save result to file (important!)\n",
    "with open('benchmark_bier_cubic', 'wb') as file:\n",
    "        pickle.dump(result_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Validation\n",
    "\n",
    "You may need to terminate kernel to run the codes below. There are OpenMP issues with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "time_points = np.arange(0,25,0.01)\n",
    "with open('test_pred.npy', 'rb') as file:\n",
    "    test_pred = np.load(file)\n",
    "\n",
    "def f(x,t):\n",
    "    \"\"\"\n",
    "    Return the derivatives (RHS of the ODE)\n",
    "    This is a linear system with the form f = A x\n",
    "    Args:\n",
    "    x -- a 2 x 1 vector of measurements\n",
    "    \"\"\"\n",
    "    A = np.array([[-0.1, 2], [-2,-0.1]]) # 2 x 2\n",
    "\n",
    "    return np.ravel(np.matmul(A,x.reshape(-1, 1)**3))\n",
    "\n",
    "# create time points\n",
    "time_points = np.arange(0,25,0.01)\n",
    "\n",
    "# specify initial conditions\n",
    "x0 = np.array([2,0])\n",
    "\n",
    "simulated_x = odeint(f, x0, time_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for species in range(2):\n",
    "    plt.subplot(2,1,species + 1)\n",
    "    plt.plot(time_points, simulated_x[:, species], 'r-', label='Simulated data')\n",
    "    plt.plot(time_points, test_pred[:,species], 'k--', label='Predicted dynamics')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('S_' + str(species))\n",
    "    plt.legend()\n",
    "    \n",
    "plt.title('Trajectories of 2D Cubic Harmonic Oscillator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase plane\n",
    "\n",
    "plt.plot(simulated_x[:,0], simulated_x[:,1], label='simulated_data')\n",
    "plt.legend()\n",
    "plt.plot(test_pred[:,0], test_pred[:,1], 'm+', label='predicted dynamics')\n",
    "plt.legend()\n",
    "plt.title('Phase plane of 2D Cubic Harmonic Oscillator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
