{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Dynamic Modeling with Machine Learning\n",
    "\n",
    "**Advanced Computational Systems**  \n",
    "_Kevin Siswandi_  \n",
    "Biological Information Processing Group  \n",
    "\n",
    "This notebook will walk you through the process of discovering dynamics from data. To do this, we will make use of a synthetic dataset (virtual strains) that were generated to mimic strains of engineered *E. Coli* for Limonene production. The general outline is:\n",
    "1. Data Analysis\n",
    "2. Creation of training data + data augmentation\n",
    "3. Pipeline building and model training\n",
    "4. Error analysis (derivative and integrated dynamics)\n",
    "\n",
    "Prerequisites:\n",
    "- install data science and scientific computing stack: `pandas`, `numpy`, and `scipy`\n",
    "- download the dataset `limonene_train.csv` and `limonene_test.csv`\n",
    "- install visualization tools: `seaborn` and `matplotlib`\n",
    "- install machine learning libraries: `sklearn`, `tpot`\n",
    "\n",
    "Parts that you need to complete are marked with `#TODO`. For more information regarding the dataset and methods, see:\n",
    "* [Weaver et al., 2015](https://pubmed.ncbi.nlm.nih.gov/24981116/) in Wiley Biotechnology and Bioengineering\n",
    "* [Costello & Martin, 2018](https://www.nature.com/articles/s41540-018-0054-3) in npj Systems Biology and Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "The first step is to load the data and take a look at some of its basic properties. Answer the following questions:\n",
    "\n",
    "**Questions**:\n",
    "1. How many test and training strains are there in the dataset?\n",
    "2. For each strain, how many time points are there?\n",
    "3. How many variables are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/limonene_train.csv' does not exist: b'data/limonene_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-92700c8575be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: change the path to where you save the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/limonene_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/limonene_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: exploratory data analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/limonene_train.csv' does not exist: b'data/limonene_train.csv'"
     ]
    }
   ],
   "source": [
    "# TODO: change the path to where you save the dataset\n",
    "train_data = pd.read_csv('data/limonene_train.csv')\n",
    "test_data = pd.read_csv('data/limonene_test.csv')\n",
    "\n",
    "# TODO: exploratory data analysis\n",
    "# how many strains are there in the training data and test data\n",
    "# how many time points are there for each strain\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# display the first 5 rows\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will investigate the distribution of the data by generating some plots. \n",
    "\n",
    "**Question**: Do the proteins/enzymes possess the same distribution? To answer this question, you can use `distplot(...)` from seaborn to plot the distribution of every enzyme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtoB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7098f9ec6f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# list of metabolites as targets\n",
    "targets = ['Acetyl-CoA','Acetoacetyl-CoA','HMG-CoA','Mev','MevP','MevPP','IPP','DMAPP','GPP','Limonene']\n",
    "\n",
    "# list of enzymes as features\n",
    "features = ['AtoB','HMGR','HMGS','MK','PMK','PMD','Idi','GPPS','LS']\n",
    "\n",
    "# TODO: plot the distribution of every protein\n",
    "\n",
    "## YOUR CODE HERE\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    sns.distplot(train_data[feature])\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, the metabolites are generated from the same initial condition (0.2), except Limonene, which has initial condition of zero. This choice is typical of bioengineering because the objective is to get Limonene as the useful product (as biofuel). The metabolite concentrations are obtained by solving a Michaelis-Menten based system of Ordinary Differential Equations with the initial conditions as mentioned.\n",
    "\n",
    "**Question**: create a trace plot of every metabolite, using `tsplot` from seaborn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Acetoacetyl-CoA', 'Acetyl-CoA', 'AtoB', 'DMAPP', 'GPP', 'GPPS',\n",
      "       'HMG-CoA', 'HMGR', 'HMGS', 'IPP', 'Idi', 'LS', 'Limonene', 'MK', 'Mev',\n",
      "       'MevP', 'MevPP', 'PMD', 'PMK'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TODO: plot the distribution or time-series (with uncertainty) of the metabolites\n",
    "\n",
    "## YOUR CODE HERE\n",
    "species = train_data.columns[2:]\n",
    "print(species)\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic modeling:\n",
    "$$ \\dot{x} = f(x) $$\n",
    "Solve by numerical integration to get $x(t)$. We want to $f(x)$ straight from data. To do this:\n",
    "1. Create training data by computing $f(x)$, the derivative as the output/target, and $x$ as the features\n",
    "2. train a machine learning model using $x$ as features and the computed derivatives as targets\n",
    "3. Solve the initial value problem using a numerical integration scheme.\n",
    "\n",
    "Here, we learn the dynamics $f(x)$ from data using machine learning.\n",
    "\n",
    "Subsetting data frame\n",
    "```\n",
    "df_subset = df.loc[df[0:index]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "As the first step, we will transform the data that you've just looked at into a form that is suitable for training machine learning models for learning dynamics. In the cell below, you will implement a function for generating dataset that can be used for training a model to learn dynamics. In addition, this function will also artificially create additional data points for training (i.e. data augmentation). This consists of the following steps:\n",
    "1. smoothing the noisy time-series data\n",
    "2. filtering the smoothed data to obtain interpolated measurements\n",
    "3. computing the derivatives numerically at the interpolated points\n",
    "\n",
    "Steps 1-2 are also data augmentation procedure for getting sufficient training data, while step 3 creates the target/output for the machine learning model. **Your task** is to complete the `generate_dataset(...)` function below, taking into account the following:\n",
    "* computation of the time step for interpolation -- to generate 100 data points between start and end time, what is the time step needed between every time point?\n",
    "* use `np.linspace` to generate the (e.g. 100, more generally `n_dim`) data points between start time and end time.\n",
    "* apply `savgol_filter` to the interpolated points\n",
    "* compute the gradients of the metabolites numerically using `np.gradient`.\n",
    "* create a multi-index dataframe -- filtered data as features and computed gradients as targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write a function to create and augment the training data\n",
    "\n",
    "def generate_dataset(data, strain_list, feature_list, target_list, n_dim):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate and augment the training data {X, y} for model fitting, using savgol filter as the smoothing method.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    data -- time-series data frame of measurements, with 'Strain' as the index\n",
    "    strain_list -- list of unique strains in `data`\n",
    "    feature_list -- list of features to be used\n",
    "    target_list -- list of targets\n",
    "    n_dim -- number of data points to generate via interpolation\n",
    "    \n",
    "    Returns:\n",
    "    ml_data -- a pandas multi-index dataframe containing features x and targets y.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ml_data = pd.DataFrame()\n",
    "    \n",
    "    for strain in strain_list:\n",
    "        measurement_data = {}\n",
    "\n",
    "        # Interpolate -> Filter -> Add to the table\n",
    "        for measurement in feature_list + target_list:\n",
    "\n",
    "            # extract measurement for the specific strain\n",
    "            measurement_series = data.loc[strain][measurement]\n",
    "            T = data.loc[strain]['Hour'] # series of time points\n",
    "            \n",
    "            ## TODO: extract the start time and end time and the time step\n",
    "            minT,maxT = None # start time and end time\n",
    "            delT = None # time step for interpolation\n",
    "        \n",
    "            # Interpolate data\n",
    "            interpolation = interp1d(T,\n",
    "                                     measurement_series.tolist(),\n",
    "                                     kind='linear')\n",
    "            \n",
    "            # TODO: generate time points to interpolate over using np.linspace\n",
    "            time_points = None\n",
    "            \n",
    "            # Consider the interpolated data over time\n",
    "            interpolated_measurement = interpolation(time_points)\n",
    "            \n",
    "            # TODO: apply savgol filter to interpolated measurement, using window length of 7 and polyorder of 2\n",
    "            filtered_measurement = None\n",
    "\n",
    "            # TODO: fill in the data to a multi-index data frame\n",
    "            if measurement in feature_list:\n",
    "                # use the filtered measurement of this enzyme as features\n",
    "                measurement_data[('feature',measurement)] = None # YOUR CODE HERE\n",
    "            if measurement in target_list:\n",
    "                # use the filtered measurment of this metabolite as a feature\n",
    "                measurement_data[('feature',measurement)] = None # YOUR CODE HERE\n",
    "                # additionally compute gradients of the filtered measurement and use it as target\n",
    "                measurement_data[('target',measurement)] = None # YOUR CODE HERE\n",
    "   \n",
    "        # Create a table\n",
    "        strain_data = pd.DataFrame(measurement_data,\n",
    "                                   index=pd.MultiIndex.from_product([[strain],np.linspace(minT,maxT,n_dim)],\n",
    "                                   names=['Strain', 'Time']))\n",
    "        ml_data = pd.concat([ml_data,strain_data])\n",
    "        \n",
    "    return ml_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we assume that the system is described by the autonomous ordinary differential equation:\n",
    "\n",
    "$$ \\dot{m} = f(m, p)$$\n",
    "\n",
    "where m is the vector of metabolite concentrations and p is the vector of protein concentrations. Now, our goal is to train a model to learn the dynamics $f$, instead of constructing a system of ODEs from knowledge of the pathway mechanisms. This can be done by training a machine learning model using the metabolite and protein concentrations as features and the derivatives as the targets. To do this, we will apply the `generate_dataset(...)` function above to create training data that consists of pairs of features and targets (derivatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the dataframe is indexed by the strain\n",
    "train_data = train_data.set_index('Strain')\n",
    "test_data = test_data.set_index(\"Strain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if your implementation is correct, run the function `generate_dataset(...)` that you have written above using the dataset provided. Note that you need to extract the strain list (both for training and test data) from the original data frame.\n",
    "\n",
    "**HINT**: every strain is identified with a number in the 'Strain' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract training and test strains\n",
    "te_strains = None\n",
    "tr_strains = None\n",
    "\n",
    "# TODO: apply the function above to create the training and test data\n",
    "# choose an appropriate data points to generate (recommended: 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment to check the generated training dataset\n",
    "\n",
    "# display(ml_train)\n",
    "# display(ml_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Model Building\n",
    "\n",
    "By now, you have read in the dataset, performed some EDA and created a suitable training data. Now, let's see how we can learn dynamics from the data. The easiest way to do this is to simply train a TPOT model, which will automate all the process (see below). However, let's first build a manual pipeline and then use TPOT afterwards. We will use three representative classes of machine learning models:\n",
    "1. Random Forest\n",
    "2. Neural Network\n",
    "3. Linear Regression\n",
    "\n",
    "For every model, there would be slightly different preprocessing steps needed (this will be automated by TPOT later). When building a manual pipeline, you can use `Pipeline(...)` from `sklearn`. You need to complete the following.\n",
    "+ create a random forest regressor with a sufficient number of estimators (e.g. 20)\n",
    "+ create a pipeline consisting of:\n",
    "    - standard scaling\n",
    "    - a linear regressor that is bagged to improve fit. Use `BaggingRegressor(...)` with ridge regression as the base estimator.\n",
    "+ create a pipelinne consiting of:\n",
    "    - standard scaling\n",
    "    - a neural network (multi-layer perceptron) regressor. Use `MLPRegressor(...)` with 4 hidden layers (each of size 5), adam solver, tanh activation, and adaptive learning rate.\n",
    "    \n",
    "If some of the terms above are unfamiliar to you, check out the following readings:\n",
    "* [standard scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) -- centering mean to zero and scaling to unit variance\n",
    "* [bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html): bootstrap aggregation -- training multiple classifiers on datasets generated from the same (bootstrapped) distribution and aggregating them afterwards\n",
    "* [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) -- linear regression with L2 regularization\n",
    "* [Adam optimization](https://arxiv.org/abs/1412.6980)\n",
    "* [adaptive learning rate](https://wiki.tum.de/display/lfdv/Adaptive+Learning+Rate+Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features are the multi-dimensional time-series concentrations\n",
    "# the target is the derivative of the dynamics\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "# TODO: create a random forest model with 20 estimators\n",
    "rf_model = None\n",
    "\n",
    "# TODO: create a pipeline consisting of: standard scaling of the data + ridge regression\n",
    "lr_model = None\n",
    "\n",
    "# TODO: create a pipeline \n",
    "nn_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model is trained using inputs as follows:\n",
    "- the features are the multi-dimensional time-series concentrations\n",
    "- the target is the derivative of the dynamics\n",
    "\n",
    "Here, the target variables are the derivatives of the metabolites in the reaction. We are going to implement this in such a way so that each target variable is trained with a distinct model. To achieve this, it's important to clone the model passed in to the function with `clone` function. In the next cell, **your task** will be to create a function that will:\n",
    "* perform the training process for the models that we specify\n",
    "* return two dictionaries:\n",
    "    - a dictionary containing a trained model for every target variable\n",
    "    - a dictonary containing the cross-validation score for each target\n",
    "* optionally save the cross-validation plot to file\n",
    "\n",
    "**Tips**: When executing `train_data(..)`, make sure to specify a figure path and create the directory if it doesn't exist. Otherwise, there will be an error returned! This is important because the training takes a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write a function that performs training\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "figure_path = './plots/' # make sure that this directory exists!\n",
    "\n",
    "def train_data(data,model,plot=False,model_type=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the input data {X, y}.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    data -- multi-index dataframe of time-series measurements, preprocessed by interpolating and filtering\n",
    "    model -- a selected machine learning model\n",
    "    plot -- decide to plot the result or not\n",
    "    model_type -- determine the input model\n",
    "    \n",
    "    Returns:\n",
    "    model_dict -- a trained model dictionary for each target\n",
    "    score_dict -- a training score dictionary or each target\n",
    "    \n",
    "    \"\"\"\n",
    "            \n",
    "    model_dict = {}\n",
    "    score_dict = {}\n",
    "\n",
    "    avg_score = 0\n",
    "    n = 0\n",
    "\n",
    "    for target_idx in data.columns:\n",
    "    \n",
    "        # All we want to train are targets\n",
    "        if target_idx[0] == 'feature':\n",
    "            continue\n",
    "        target = target_idx[1]\n",
    "        \n",
    "        # TODO: create the data matrix X and the target vector y\n",
    "        X = None # YOUR CODE HERE\n",
    "        y = None # YOUR CODE HERE\n",
    "        \n",
    "        if model_type == 'tpot':\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "        \n",
    "        # TODO: train the model\n",
    "        # IMPORTANT: clone the model to train a different one for each target\n",
    "        if model_type == 'tpot':\n",
    "            # if TPOT, use the best pipeline found\n",
    "            model_dict[target] = None # YOUR CODE HERE\n",
    "        else:\n",
    "            # if RF/NN/LR, simply fit X and y\n",
    "            model_dict[target] = None # YOUR CODE HERE\n",
    "        \n",
    "        # Plot results, if required\n",
    "        if plot:\n",
    "            \n",
    "            # The training_plot function is defined below for you to complete\n",
    "            CV_plot = training_plot(model_dict[target],\n",
    "                                    target,X,y,\n",
    "                                    cv=ShuffleSplit())\n",
    "            \n",
    "            axis = plt.gca()\n",
    "            axis.set_ylim([-0.1, 1.1])\n",
    "            \n",
    "            strip_target = ''.join([char for char in target if char != '/'])\n",
    "            print(strip_target)\n",
    "            \n",
    "            CV_plot.savefig(figure_path + strip_target + '_' + model_type + '_CV_plot.pdf',transparent=False)\n",
    "            \n",
    "            plt.show()\n",
    "    \n",
    "        # TODO: evaluate the model score\n",
    "        # Every model in sklearn API has its own default scoring metric (see the respective docs), but can be easily accesed via the score method\n",
    "        score = None ## YOUR CODE HERE\n",
    "            \n",
    "        print('Target: {}, CV Pearson R2 coefficient: {:f}'.format(target,score))\n",
    "        score_dict[target] = score\n",
    "    \n",
    "    # TODO: compute the average score over all targets\n",
    "    avg_score = None #YOUR CODE HERE\n",
    "    print('Average training score:', avg_score)\n",
    "    \n",
    "    return model_dict,score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function to plot training curves below\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def training_plot(estimator,title,X,y,\n",
    "                  cv=None,n_jobs=1, \n",
    "                  train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a plot in training process.\n",
    "\n",
    "    Arguements:\n",
    "    \n",
    "    estimator -- a machine learning model.\n",
    "    title -- a title for the chart.\n",
    "    X -- array of features.\n",
    "    y -- target array corresponded to X.\n",
    "    cv -- a cross-validation generator.\n",
    "    n_jobs : a number of jobs to run in parallel.\n",
    "    \n",
    "    Return:\n",
    "    plt -- a desired plot.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "        \n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    # TODO: call the learning_curve function to get scores for different training sizes\n",
    "    train_sizes = None\n",
    "    train_scores = None\n",
    "    test_scores = None\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: compute the mean and standard deviation of the scores\n",
    "    train_scores_mean = None #YOUR CODE HERE\n",
    "    train_scores_std = None #YOUR CODE HERE\n",
    "    \n",
    "    test_scores_mean = None #YOUR CODE HERE\n",
    "    test_scores_std = None #YOUR CODE HERE\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Model Building\n",
    "\n",
    "Now, let's automate the machine learning pipeline with TPOT. TPOT finds the best pipeline with programming and cross-validation: it will automate the most tedious part of machine learning by exploring several possible pipelines to find the best one for the specific dataset:\n",
    "\n",
    "![An example Machine Learning pipeline](https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/tpot-ml-pipeline.png)\n",
    "\n",
    "Once TPOT is finished searching (or you get tired of waiting), it returns the best scikit-learn pipeline it found so you can tinker with the pipeline from there.\n",
    "\n",
    "![An example TPOT pipeline](https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/tpot-pipeline-example.png)\n",
    "\n",
    "All images above are courtesy of [Epistasis Lab](https://github.com/EpistasisLab/tpot). More Hints:\n",
    "- If you need a config dict, take a look at https://github.com/EpistasisLab/tpot/blob/master/tpot/config/regressor.py\n",
    "- specify a max time limit of 30 minutes to avoid the algorithm running for too long\n",
    "- use `ShuffleSplit` from scikit-learn for the cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from source.tpot_config import tpot_config_dict\n",
    "\n",
    "# TODO: create a TPOT regressor\n",
    "tpot_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: call the train_data function to train a random forest model (may take ~15 mins to compute)\n",
    "\n",
    "model_type = 'random_forest'\n",
    "rf_dict= None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  8.0108642578125e-05\n"
     ]
    }
   ],
   "source": [
    "# TODO: call the train_data function to train a neural network model (may take ~15 mins to compute)\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "model_type = 'neural_network'\n",
    "nn_dict = None # YOUR CODE HERE\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Elapsed time: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  8.320808410644531e-05\n"
     ]
    }
   ],
   "source": [
    "# TODO: call the train_data function to train a linear regression model (may take ~15 mins to compute)\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "model_type = 'linear_regression'\n",
    "lr_dict = None # YOUR CODE HERE\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Elapsed time: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  7.390975952148438e-05\n"
     ]
    }
   ],
   "source": [
    "# TODO: call the train_data function to train an automated TPOT model\n",
    "# (warning, this may take several hours! skip this and come back later if needed after you finish the other sections)\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "model_type = 'tpot'\n",
    "tpot_dict = None # YOUR CODE HERE\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Elapsed time: \", t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "1. You may have noticed that random forest performs poorly for predicting Limonene, but shows OK performance for predicting other metabolites. Why do you think this is so?\n",
    "2. Try to improve the performance of the neural network by doing some hyperparameter tuning (e.g. reducing the number of hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function to compute RMSE error for every metabolite and the total RMSE\n",
    "# make sure that the directory specified in figure_path exists! otherwise the execution will return an error.\n",
    "import math\n",
    "\n",
    "figure_path = './plots/' # make sure that this directory exists!\n",
    "\n",
    "def compute_error(data,model_dict,plot=False,model_type=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    To check the error of predicted derivative.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    data -- time-series data of measurements, preprocessed by interpolating and filtering\n",
    "    model_dict -- a dictionary of trained models or each target\n",
    "    plot -- decide to plot the result or not\n",
    "    model_type -- determine the input model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # list of errors\n",
    "    # this contains the error for every metabolite (target)\n",
    "    error_list = []\n",
    "\n",
    "    for target in model_dict:\n",
    "        # Extract input target\n",
    "        y_test = data[('target',target)].values\n",
    "    \n",
    "        # Extract predicted target\n",
    "        feature_list = [('feature',feature) for feature in data['feature'].columns]\n",
    "        target_data = data[feature_list]\n",
    "        y_prediction = model_dict[target].predict(target_data.values)\n",
    "    \n",
    "        # TODO: Compute squared error and append it to the list of errors\n",
    "        ## YOUR CODE HERE\n",
    "        error = None\n",
    "        error_list.append(error)\n",
    "        \n",
    "        # Compute mean and standard deviation of squared error\n",
    "        ## YOUR CODE HERE\n",
    "        mu = None\n",
    "        sigma = None\n",
    "        print(target,'RMSE:',mu,'standard deviation:',sigma)\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(13,4))\n",
    "            plt.subplot(121)\n",
    "            sns.distplot(error)\n",
    "            \n",
    "            plt.title(target + ' Derivative '+ 'Error Residual Histogram')\n",
    "            plt.xlabel('Derivative Residual Error')\n",
    "            plt.ylabel('Probability Density')\n",
    "    \n",
    "            plt.subplot(122)\n",
    "            error_plot(target,y_prediction,y_test) # this function is provided below\n",
    "    \n",
    "            strip_target = ''.join([char for char in target if char != '/'])\n",
    "            plt.savefig(figure_path + strip_target +'_'+ model_type + '_Error_Residuals.pdf')\n",
    "            plt.show()\n",
    "\n",
    "    # TODO: compute total error from the error list\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    mu = None\n",
    "    sigma = None\n",
    "    print('Total Derivative','Mean Error:',mu,'Error Standard Deviation:',sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function error plot is provided to you below.\n",
    "\n",
    "def error_plot(name,pred,real):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a plot from detecting error of derivatives.\n",
    "\n",
    "    Arguements:\n",
    "    \n",
    "    name -- a name for the title.\n",
    "    pred -- a list of predicted derivatives\n",
    "    real -- a list of actual derivatives\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    plt.scatter(pred,real)\n",
    "    plt.title(name + ' Predicted vs. Actual')\n",
    "    \n",
    "    axis = plt.gca()\n",
    "    axis.plot([-120,120], [-120,120], ls=\"--\", c=\".3\")\n",
    "    \n",
    "    padding_y = (max(real) - min(real))*0.1\n",
    "    plt.ylim(min(real)-padding_y,max(real)+padding_y)\n",
    "    \n",
    "    padding_x = (max(pred) - min(pred))*0.1\n",
    "    plt.xlim(min(pred)-padding_x,max(pred)+padding_x)\n",
    "    \n",
    "    plt.xlabel('Predicted ' + name)\n",
    "    plt.ylabel('Actual ' + name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TODO: call the compute error function for the random forest model,\n",
    "# using the test data and also training (in-sample) data\n",
    "\n",
    "#compute_error(ml_test,rf_dict,plot=True,model_type='random_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: call the compute error function for the neural network model,\n",
    "# the linear regression model\n",
    "# using the test data and also training (in-sample) data\n",
    "\n",
    "#compute_error(ml_test,lr_dict,plot=True,model_type='linear_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: call the compute error function for the tpot model\n",
    "# using the test data and also training (in-sample) data\n",
    "\n",
    "#compute_error(ml_test,tpot_dict,plot=True,model_type='tpot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to use this trained model to make predictions. To do that, we will solve the initial value problem by numerical integration. The function for solving the IVP is provided for you. The general idea is:\n",
    "1. Create the derivative function from the learned model\n",
    "2. solve the IVP with the derivative function and initial conditions\n",
    "\n",
    "Below, you will find two functions:\n",
    "1. `int_ode(...)` for solving the initial value problem\n",
    "2. `ml_ode(...)` for defining the derivative dynamics learned by machine learning (representing the 'ODE equation'). This function will be passed to the `int_ode` function to be integrated.\n",
    "\n",
    "**Your task** is to apply the provided functions `ml_ode(...)` to integrate the dynamics. In our setting, the function that describes the dynamics of the metabolites $f(m,p) = \\dot{m}$ takes the protein concentrations as given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are provided with the function to integrate below\n",
    "from scipy.integrate import ode\n",
    "\n",
    "def int_ode(g,y0,times,solver='scipy'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Integration function corresponded to the ode, generated by ml_ode.\n",
    "    \n",
    "    Arguments:\n",
    "    f -- an ode equation to be integrated\n",
    "    y0 -- an initial condition as a list of concentrations\n",
    "    times -- a list of time coordinate\n",
    "    solver -- string of package used for ode solver\n",
    "    \n",
    "    Return:\n",
    "    x -- a solution of the ode problem\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if solver == 'assimulo':\n",
    "        from assimulo.problem import Explicit_Problem\n",
    "        from assimulo.solvers import Dopri5\n",
    "        \n",
    "        # Set up ODE\n",
    "        rhs = lambda t,x: g(x,t)\n",
    "        model = Explicit_Problem(rhs,y0,min(times))\n",
    "        sim = Dopri5(model)\n",
    "        \n",
    "        # Preform integration\n",
    "        _,x = sim.simulate(max(times),max(times))\n",
    "        return np.array(x)[np.array(times).astype(int)].tolist()\n",
    "    \n",
    "    elif solver == 'scipy':\n",
    "        # Set up ODE\n",
    "        f = lambda t,x: g(x,t)\n",
    "        r = ode(f).set_integrator('dopri5',\n",
    "                                  nsteps=1e4,\n",
    "                                  atol=1e-3)\n",
    "    \n",
    "        r.set_initial_value(y0,times[0])\n",
    "    \n",
    "        #widgets.FloatProgress(min=0, max=max(times))\n",
    "    \n",
    "        # Preform integration\n",
    "        x = [y0,]\n",
    "        currentT = times[0]\n",
    "        max_delT = 10\n",
    "    \n",
    "        for nextT in times[1:]:\n",
    "        \n",
    "            while r.t < nextT:\n",
    "            \n",
    "                if nextT-currentT < max_delT:\n",
    "                    dt = nextT-currentT\n",
    "                else:\n",
    "                    dt = max_delT\n",
    "                \n",
    "                value = r.integrate(r.t + dt)\n",
    "                currentT = r.t\n",
    "\n",
    "                f.value = currentT\n",
    "            \n",
    "            x.append(value)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are provided with the function to define the derivatives below\n",
    "\n",
    "def ml_ode(model_dict, data, targets, features, time_index='Hour'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Set up an ODE.\n",
    "    \n",
    "    Arguments:\n",
    "    model_type -- a string for desired model\n",
    "    data -- raw time-series data of measurements\n",
    "    targets -- list of targets\n",
    "    features -- list of features\n",
    "    time_index -- a string labelel for time index of the input data\n",
    "    \n",
    "    Return:\n",
    "    f - an output ODE\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create interpolations for each feature\n",
    "    ml_interpolation = {}\n",
    "    \n",
    "    for feature in data.columns:\n",
    "        feature_columns = feature\n",
    "        \n",
    "        if isinstance(feature,tuple):\n",
    "            if feature[0]=='feature':\n",
    "                feature = feature[1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if feature in features:    \n",
    "            X,y = data.reset_index()[time_index].tolist(), data[feature_columns].tolist()\n",
    "\n",
    "            ml_interpolation[feature] = interp1d(X,y)\n",
    "            \n",
    "    # Define the function to be integrated\n",
    "    def f(x,t):\n",
    "        x_dot = []\n",
    "        \n",
    "        # Create derivatives for each target\n",
    "        for target in targets:\n",
    "            x_pred = []\n",
    "            \n",
    "            # loop over all species\n",
    "            for feature in data.columns:\n",
    "                if isinstance(feature,tuple):\n",
    "                    if feature[0]=='feature':\n",
    "                        feature = feature[1]\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                if feature in features:\n",
    "                    x_pred = np.append(x_pred, ml_interpolation[feature](t))\n",
    "                elif feature in targets:\n",
    "                    x_pred = np.append(x_pred, x[targets.index(feature)])\n",
    "                \n",
    "            model_prediction = model_dict[target].predict(x_pred.reshape(1,-1))\n",
    "            x_dot = np.append(x_dot,model_prediction)   \n",
    "            \n",
    "        return x_dot\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function to integrate the dynamics and predict time points\n",
    "import random\n",
    "from scipy.integrate import quad\n",
    "figure_path = './plots/'\n",
    "\n",
    "def predict_integrate(ts_data,tr_data,model_dict,targets,features,pathway,\n",
    "              plot=False,model_type=None,solver='scipy'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Integrate the learned 'ODE' and use it for simulations\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    tr_data -- raw time-series data of measurements used for training\n",
    "    ts_data -- raw time-series data of measurements used for testing\n",
    "    model_dict -- a dictionary of trained models or each target\n",
    "    pathway -- a selected pathway\n",
    "    targets -- list of targets\n",
    "    features -- list of features\n",
    "    plot -- decide to plot the result or not\n",
    "    model_type -- determine the input model\n",
    "    solver -- string of package used for ode solver\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rmse_average = []\n",
    "    rmse_percent = []\n",
    "    \n",
    "    ts = ts_data \n",
    "    \n",
    "    # Get a randomed strain\n",
    "    strains = ts.index.get_level_values(0).unique().tolist()\n",
    "    strain = random.sample(strains,1)\n",
    "    \n",
    "    test_data = ts.loc[strain]\n",
    "        \n",
    "    # TODO: get the initial conditions from test_data\n",
    "    y0 = None\n",
    "    \n",
    "    # TODO: call ml_ode function to construct the 'ODE'\n",
    "    g = None\n",
    "\n",
    "    # Get the time points\n",
    "    times = test_data.reset_index()['Hour'].tolist()\n",
    "        \n",
    "    # TODO: call int_ode to integrate the 'ODE' g\n",
    "    fit = None\n",
    "        \n",
    "    # Format the output as a table\n",
    "    fit_data = pd.DataFrame(fit, \n",
    "                            index=times, \n",
    "                            columns = targets).rename_axis('Hour')\n",
    "    \n",
    "    # Set up real data and predicted targets\n",
    "    real = test_data[targets]\n",
    "    pred = fit_data\n",
    "        \n",
    "    # Display them\n",
    "    print('Real data:')\n",
    "    display(real)\n",
    "    print('Predicted data:')\n",
    "    display(pred)\n",
    "        \n",
    "        \n",
    "    for metabolite in fit_data.columns:\n",
    "        t,X = times, real[metabolite].tolist()\n",
    "        real_fcn = interp1d(t,X)\n",
    "        pred_fcn = interp1d(times,pred[metabolite])\n",
    "            \n",
    "        '''\n",
    "        Optional \n",
    "        times =  real[metabolite].dropna().index.tolist()\n",
    "        real_fcn = interp1d(times,real[metabolite].dropna())\n",
    "        pred_fcn = interp1d(times,pred[metabolite].loc[times])\n",
    "        '''\n",
    "            \n",
    "        # Calculate RMSE average\n",
    "        integrand = lambda t: (real_fcn(t) - pred_fcn(t))**2\n",
    "        rmse = math.sqrt(quad(integrand,min(times),max(times),limit=200)[0])\n",
    "        rmse_average.append(rmse)\n",
    "            \n",
    "         # Calculate RMSE percentage\n",
    "        percent_integrand = lambda t: abs(real_fcn(t) - pred_fcn(t))/(real_fcn(t)*max(times))\n",
    "        rmsep = math.sqrt(quad(percent_integrand,min(times),max(times),limit=200)[0])\n",
    "        rmse_percent.append(rmsep)\n",
    "        \n",
    "        print('ML Fit:',metabolite,rmse,\n",
    "              'RMSE percentage:',rmsep*100)\n",
    "    \n",
    "    print('ML model aggregate error')\n",
    "    print('Average RMSE:',sum(rmse_average)/len(rmse_average))\n",
    "    print('Total percentage error:',sum(rmse_percent)/len(rmse_percent)*100)\n",
    "        \n",
    "    if plot:\n",
    "        tr = tr_data\n",
    "        fitT = list(map(list, zip(*fit)))\n",
    "        \n",
    "        # Create interpolation functions for each feature\n",
    "        interp_f = {}\n",
    "            \n",
    "        for feature in test_data.columns:\n",
    "            t,X = test_data.reset_index()['Hour'].tolist(), test_data[feature].tolist()\n",
    "            interp_f[feature] = interp1d(t,X)\n",
    "        \n",
    "        plt.figure(figsize=(12,8))\n",
    "        \n",
    "        common_targets = ['Acetyl-CoA','Acetoacetyl-CoA','HMG-CoA','Mev','IPP', 'Limonene']\n",
    "        for i,target in enumerate(common_targets):\n",
    "            plt.subplot(2,3,i+1)\n",
    "            \n",
    "            for strain in tr_strains:\n",
    "                strain_interp_f = {}\n",
    "                strain_df = tr.loc[strain]\n",
    "                \n",
    "                X,y = strain_df.reset_index()['Hour'].tolist(), strain_df[target].tolist()\n",
    "                strain_interp_f[target] = interp1d(X,y)\n",
    "                \n",
    "                actual_data = [strain_interp_f[target](t) for t in times]\n",
    "                \n",
    "                train_line, = plt.plot(times,actual_data,'r--')\n",
    "                    \n",
    "            actual_data = [interp_f[target](t) for t in times]\n",
    "            \n",
    "            pos_pred = [max(fitT[i][j],0) for j,t in enumerate(times)]\n",
    "            prediction_line, = plt.plot(times,pos_pred)\n",
    "            \n",
    "            test_line, = plt.plot(times,actual_data,'g--')\n",
    "            \n",
    "            plt.ylabel(target)\n",
    "            plt.xlabel('Time [h]')\n",
    "            plt.xlim([0,72])\n",
    "    \n",
    "        if pathway=='isopentenol':\n",
    "            product = 'Isopentenol'\n",
    "        elif pathway=='limonene':\n",
    "            product = 'Limonene'\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.90)\n",
    "        plt.subplots_adjust(bottom=0.12)\n",
    "        plt.suptitle('Prediction of ' + product + ' Strain Dynamics', fontsize=18)\n",
    "        plt.figlegend((train_line,test_line,prediction_line), \n",
    "                      ('Training Set Data','Test Data','Machine Learning Model Prediction'), \n",
    "                      loc = 'lower center', ncol=5, labelspacing=0. ) \n",
    "            \n",
    "        plt.savefig(figure_path + product + model_type +'_prediction.eps', format='eps', dpi=600)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: call the function predict_integrate to integrate dynamics and make predictions\n",
    "#predict_integrate(test_data,train_data,lr_dict,targets,features,'limonene',plot=True,model_type='linear_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have seen how machine learning can be used for empirical dynamic modeling in systems biology. Next, you can explore other ways to use what you have For example, you can use the learned model to make simulations and explore the metabolomics/proteomics phase space. You can gain further insights, e.g. by performing PCA and visualizing the simulations in 2-D principal components phase space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
